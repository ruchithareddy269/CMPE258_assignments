{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file consists of:  \n",
        "\n",
        "- Designing the model with Pytorch Lightning\n",
        "\n",
        "- This Neural network model consists of 4 layers : 1 input, 2 hidden and 1 output"
      ],
      "metadata": {
        "id": "6NhmtamoOQyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Pytorch Lightning"
      ],
      "metadata": {
        "id": "cVKzOOJKfeEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76U22rnbgU_",
        "outputId": "f640b5ab-ea6a-4df6-8970-f760412fcb0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.10.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch_lightning) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch_lightning)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch_lightning-2.2.1 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "X2jV9ub5AsXL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are setting the model here:\n",
        "\n",
        "model consists of 4 layers : 1 input layer, 2 hidden layers and 1 output layer\n",
        "\n",
        "input is 28*28 handwritten digit images\n",
        "\n",
        "since digits are 0-9, we have 10 output labels\n"
      ],
      "metadata": {
        "id": "CH_qHtuHfi7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MNISTClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height)\n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 128)\n",
        "    self.layer_3 = torch.nn.Linear(128, 256)\n",
        "    self.layer_4 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.size()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 4\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "sPyrmKrgb4RD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing the model with Pytorch Lightning\n",
        "\n",
        "This Neural network model consists of 4 layers : 1 input, 2 hidden and 1 output"
      ],
      "metadata": {
        "id": "1O2PII6LhR6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gNRUhzNwTBg1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height)\n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 128)\n",
        "    self.layer_3 = torch.nn.Linear(128, 256)\n",
        "    self.layer_4 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.siz()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 4\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting The Data\n",
        "\n",
        "We split MNIST data to a training, validation and test split.\n",
        "\n",
        "The dataset is added to the Dataloader which handles the loading, shuffling and batching of the dataset.\n",
        "\n",
        "Data preparation:\n",
        "- Image transforms\n",
        "- Generate training, validation and test dataset splits.\n",
        "- Wrap each dataset split in a DataLoader"
      ],
      "metadata": {
        "id": "hwcAZ9ODhjD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# TRANSFORMS\n",
        "# ----------------\n",
        "# prepare transforms standard to MNIST\n",
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# ----------------\n",
        "# TRAINING, VAL DATA\n",
        "# ----------------\n",
        "mnist_train = MNIST(os.getcwd(), train=True, download=True)\n",
        "\n",
        "# train (55,000 images), val split (5,000 images)\n",
        "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "# ----------------\n",
        "# TEST DATA\n",
        "# ----------------\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "# ----------------\n",
        "# DATALOADERS\n",
        "# ----------------\n",
        "# The dataloaders handle shuffling, batching, etc...\n",
        "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=64)\n",
        "mnist_test = DataLoader(mnist_test, batch_size=64)"
      ],
      "metadata": {
        "id": "SVHR3ZqNbaDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8d7297-1450-4d42-8e26-59b8612a2c14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 139929022.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 59790569.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 42789473.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12049670.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Laoding the data using the following methods:\n",
        "- train_dataloader()\n",
        "- val_dataloader()\n",
        "- test_dataloader()\n",
        "\n",
        "Method for data preparation:\n",
        "- prepare_data()"
      ],
      "metadata": {
        "id": "eLGmL17Kh5CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def setup(self, stage):\n",
        "    # transforms for images\n",
        "    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    # prepare transforms standard to MNIST\n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "    mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "\n",
        "    self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.mnist_train, batch_size=64)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.mnist_val, batch_size=64)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self,mnist_test, batch_size=64)"
      ],
      "metadata": {
        "id": "UaxTWjYsbukm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Optimizer\n",
        "\n",
        "Using Adam optimizer for optimization.\n",
        "\n",
        "The optimizer is given the weights to optimizer when we init the optimizer.\n",
        "\n",
        "The optimizer code is added to the function configure_optimizers() in the LightningModule."
      ],
      "metadata": {
        "id": "keSSvA-RiEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "      return optimizer"
      ],
      "metadata": {
        "id": "-400Vx7PeBeT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss\n",
        "\n",
        "In this case, we want to take our logits and calculate the cross entropy loss. Since cross entropy is the same as NegativeLogLikelihood(log_softmax), we just need to add the nll_loss.\n"
      ],
      "metadata": {
        "id": "oYaSpDvUiIMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def cross_entropy_loss(logits, labels):\n",
        "  return F.nll_loss(logits, labels)"
      ],
      "metadata": {
        "id": "RyIEsZneeC-q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch Lightning, calculating loss as follows."
      ],
      "metadata": {
        "id": "8-6AzBJUiQ_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)"
      ],
      "metadata": {
        "id": "Fcc_QmQGeE7S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Training loop\n",
        "\n",
        "Determine training and validation loss"
      ],
      "metadata": {
        "id": "GDWTwVP3iWAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# -----------------\n",
        "# MODEL\n",
        "# -----------------\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height)\n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 128)\n",
        "    self.layer_3 = torch.nn.Linear(128, 256)\n",
        "    self.layer_4 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.sizes()\n",
        "\n",
        "    # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    # layer 1\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 2\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 3\n",
        "    x = self.layer_3(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    # layer 4\n",
        "    x = self.layer_4(x)\n",
        "\n",
        "    # probability distribution over labels\n",
        "    x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# ----------------\n",
        "# DATA\n",
        "# ----------------\n",
        "transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "mnist_train = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True, transform=transform)\n",
        "\n",
        "# train (55,000 images), val split (5,000 images)\n",
        "mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "mnist_test = MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "# The dataloaders handle shuffling, batching, etc...\n",
        "mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=64)\n",
        "mnist_test = DataLoader(mnist_test, batch_size=64)\n",
        "\n",
        "# ----------------\n",
        "# OPTIMIZER\n",
        "# ----------------\n",
        "pytorch_model = MNISTClassifier()\n",
        "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)\n",
        "\n",
        "# ----------------\n",
        "# LOSS\n",
        "# ----------------\n",
        "def cross_entropy_loss(logits, labels):\n",
        "  return F.nll_loss(logits, labels)\n",
        "\n",
        "# ----------------\n",
        "# TRAINING LOOP\n",
        "# ----------------\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # TRAINING LOOP\n",
        "  for train_batch in mnist_train:\n",
        "    x, y = train_batch\n",
        "\n",
        "    logits = pytorch_model(x)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "    print('train loss: ', loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # VALIDATION LOOP\n",
        "  with torch.no_grad():\n",
        "    val_loss = []\n",
        "    for val_batch in mnist_val:\n",
        "      x, y = val_batch\n",
        "      logits = pytorch_model(x)\n",
        "      val_loss.append(cross_entropy_loss(logits, y).item())\n",
        "\n",
        "    val_loss = torch.mean(torch.tensor(val_loss))\n",
        "    print('val_loss: ', val_loss.item())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6isRz8NeIh9",
        "outputId": "37e6f869-7321-410d-dcc9-0db6d8ea2be9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:  2.304131269454956\n",
            "train loss:  2.272151470184326\n",
            "train loss:  2.255650758743286\n",
            "train loss:  2.2290241718292236\n",
            "train loss:  2.2441608905792236\n",
            "train loss:  2.1794075965881348\n",
            "train loss:  2.1058895587921143\n",
            "train loss:  1.9933878183364868\n",
            "train loss:  1.9309513568878174\n",
            "train loss:  1.8956352472305298\n",
            "train loss:  1.8028937578201294\n",
            "train loss:  1.6681115627288818\n",
            "train loss:  1.6777138710021973\n",
            "train loss:  1.3864494562149048\n",
            "train loss:  1.4209160804748535\n",
            "train loss:  1.199084758758545\n",
            "train loss:  1.1193442344665527\n",
            "train loss:  1.1522703170776367\n",
            "train loss:  0.9698446989059448\n",
            "train loss:  1.0608196258544922\n",
            "train loss:  0.7497735619544983\n",
            "train loss:  0.8407434225082397\n",
            "train loss:  0.7120811343193054\n",
            "train loss:  0.9433042407035828\n",
            "train loss:  0.7276260852813721\n",
            "train loss:  0.6947061419487\n",
            "train loss:  0.6385531425476074\n",
            "train loss:  0.9029319286346436\n",
            "train loss:  0.7376767992973328\n",
            "train loss:  0.9488387703895569\n",
            "train loss:  0.6394418478012085\n",
            "train loss:  0.5648949146270752\n",
            "train loss:  0.7953107953071594\n",
            "train loss:  0.5424145460128784\n",
            "train loss:  0.645875096321106\n",
            "train loss:  0.4409174919128418\n",
            "train loss:  0.5339842438697815\n",
            "train loss:  0.6947972178459167\n",
            "train loss:  0.5181630253791809\n",
            "train loss:  0.5605049133300781\n",
            "train loss:  0.7684181332588196\n",
            "train loss:  0.6407067179679871\n",
            "train loss:  0.5609867572784424\n",
            "train loss:  0.36248913407325745\n",
            "train loss:  0.5582040548324585\n",
            "train loss:  0.6794633865356445\n",
            "train loss:  0.35526153445243835\n",
            "train loss:  0.40493112802505493\n",
            "train loss:  0.4781302511692047\n",
            "train loss:  0.4502400755882263\n",
            "train loss:  0.536647617816925\n",
            "train loss:  0.41991931200027466\n",
            "train loss:  0.29370376467704773\n",
            "train loss:  0.4570941627025604\n",
            "train loss:  0.603290319442749\n",
            "train loss:  0.36461421847343445\n",
            "train loss:  0.4639088213443756\n",
            "train loss:  0.2427968829870224\n",
            "train loss:  0.5734170079231262\n",
            "train loss:  0.5250394940376282\n",
            "train loss:  0.35505154728889465\n",
            "train loss:  0.5440927147865295\n",
            "train loss:  0.45013776421546936\n",
            "train loss:  0.5332872271537781\n",
            "train loss:  0.7746255993843079\n",
            "train loss:  0.32975929975509644\n",
            "train loss:  0.4000738263130188\n",
            "train loss:  0.45025911927223206\n",
            "train loss:  0.5769592523574829\n",
            "train loss:  0.5301051139831543\n",
            "train loss:  0.4490916430950165\n",
            "train loss:  0.40464547276496887\n",
            "train loss:  0.6204986572265625\n",
            "train loss:  0.39151591062545776\n",
            "train loss:  0.5147970914840698\n",
            "train loss:  0.5694483518600464\n",
            "train loss:  0.3246776759624481\n",
            "train loss:  0.32493191957473755\n",
            "train loss:  0.5194952487945557\n",
            "train loss:  0.4618777930736542\n",
            "train loss:  0.4626207947731018\n",
            "train loss:  0.41416454315185547\n",
            "train loss:  0.41501685976982117\n",
            "train loss:  0.33157211542129517\n",
            "train loss:  0.35040685534477234\n",
            "train loss:  0.38399213552474976\n",
            "train loss:  0.7035005688667297\n",
            "train loss:  0.3758934438228607\n",
            "train loss:  0.39072203636169434\n",
            "train loss:  0.3559911251068115\n",
            "train loss:  0.29863306879997253\n",
            "train loss:  0.2858501076698303\n",
            "train loss:  0.44103825092315674\n",
            "train loss:  0.2863827645778656\n",
            "train loss:  0.2733381986618042\n",
            "train loss:  0.338556706905365\n",
            "train loss:  0.4160647988319397\n",
            "train loss:  0.1406056433916092\n",
            "train loss:  0.1692628413438797\n",
            "train loss:  0.526194155216217\n",
            "train loss:  0.23824670910835266\n",
            "train loss:  0.3076608180999756\n",
            "train loss:  0.4372483491897583\n",
            "train loss:  0.2931983768939972\n",
            "train loss:  0.4133453965187073\n",
            "train loss:  0.2819913923740387\n",
            "train loss:  0.3936012387275696\n",
            "train loss:  0.46221059560775757\n",
            "train loss:  0.32466921210289\n",
            "train loss:  0.5218706727027893\n",
            "train loss:  0.2570653557777405\n",
            "train loss:  0.3595738410949707\n",
            "train loss:  0.5398864150047302\n",
            "train loss:  0.4277876019477844\n",
            "train loss:  0.24775119125843048\n",
            "train loss:  0.2749262750148773\n",
            "train loss:  0.3292088508605957\n",
            "train loss:  0.2973051369190216\n",
            "train loss:  0.4213813543319702\n",
            "train loss:  0.5710493326187134\n",
            "train loss:  0.30703508853912354\n",
            "train loss:  0.2805844247341156\n",
            "train loss:  0.16898393630981445\n",
            "train loss:  0.268923282623291\n",
            "train loss:  0.23472099006175995\n",
            "train loss:  0.18256962299346924\n",
            "train loss:  0.37892860174179077\n",
            "train loss:  0.3281695246696472\n",
            "train loss:  0.5713744759559631\n",
            "train loss:  0.2722128629684448\n",
            "train loss:  0.4884348213672638\n",
            "train loss:  0.33486229181289673\n",
            "train loss:  0.48471564054489136\n",
            "train loss:  0.30446168780326843\n",
            "train loss:  0.14677830040454865\n",
            "train loss:  0.4307538866996765\n",
            "train loss:  0.29643115401268005\n",
            "train loss:  0.33937403559684753\n",
            "train loss:  0.28036317229270935\n",
            "train loss:  0.16773325204849243\n",
            "train loss:  0.3391992151737213\n",
            "train loss:  0.34228870272636414\n",
            "train loss:  0.22343140840530396\n",
            "train loss:  0.22626973688602448\n",
            "train loss:  0.4157484173774719\n",
            "train loss:  0.304371178150177\n",
            "train loss:  0.7072730660438538\n",
            "train loss:  0.24754728376865387\n",
            "train loss:  0.4108593463897705\n",
            "train loss:  0.23431701958179474\n",
            "train loss:  0.29487013816833496\n",
            "train loss:  0.18710562586784363\n",
            "train loss:  0.4103662371635437\n",
            "train loss:  0.18865704536437988\n",
            "train loss:  0.3135881721973419\n",
            "train loss:  0.32855746150016785\n",
            "train loss:  0.40695619583129883\n",
            "train loss:  0.4487493932247162\n",
            "train loss:  0.35607290267944336\n",
            "train loss:  0.20447231829166412\n",
            "train loss:  0.3419479429721832\n",
            "train loss:  0.1277090162038803\n",
            "train loss:  0.3021392524242401\n",
            "train loss:  0.21604228019714355\n",
            "train loss:  0.28046348690986633\n",
            "train loss:  0.41617220640182495\n",
            "train loss:  0.242877796292305\n",
            "train loss:  0.10945724695920944\n",
            "train loss:  0.324302077293396\n",
            "train loss:  0.383462518453598\n",
            "train loss:  0.4313729405403137\n",
            "train loss:  0.2795349657535553\n",
            "train loss:  0.13835802674293518\n",
            "train loss:  0.29937559366226196\n",
            "train loss:  0.38143807649612427\n",
            "train loss:  0.22297827899456024\n",
            "train loss:  0.37024739384651184\n",
            "train loss:  0.2236362248659134\n",
            "train loss:  0.2157279998064041\n",
            "train loss:  0.1495198905467987\n",
            "train loss:  0.22921648621559143\n",
            "train loss:  0.4032462239265442\n",
            "train loss:  0.2525889575481415\n",
            "train loss:  0.15879686176776886\n",
            "train loss:  0.12941871583461761\n",
            "train loss:  0.13783304393291473\n",
            "train loss:  0.2347157746553421\n",
            "train loss:  0.15614347159862518\n",
            "train loss:  0.31843286752700806\n",
            "train loss:  0.30750811100006104\n",
            "train loss:  0.21427716314792633\n",
            "train loss:  0.22668610513210297\n",
            "train loss:  0.4466712474822998\n",
            "train loss:  0.17710399627685547\n",
            "train loss:  0.08780957013368607\n",
            "train loss:  0.33891740441322327\n",
            "train loss:  0.4358823895454407\n",
            "train loss:  0.219627246260643\n",
            "train loss:  0.290820837020874\n",
            "train loss:  0.4068116843700409\n",
            "train loss:  0.13558128476142883\n",
            "train loss:  0.25955259799957275\n",
            "train loss:  0.2013278305530548\n",
            "train loss:  0.23290561139583588\n",
            "train loss:  0.22613094747066498\n",
            "train loss:  0.24248887598514557\n",
            "train loss:  0.2725946605205536\n",
            "train loss:  0.2081555426120758\n",
            "train loss:  0.18299593031406403\n",
            "train loss:  0.24133355915546417\n",
            "train loss:  0.48408302664756775\n",
            "train loss:  0.37383338809013367\n",
            "train loss:  0.18372121453285217\n",
            "train loss:  0.31264567375183105\n",
            "train loss:  0.11950980126857758\n",
            "train loss:  0.2829083800315857\n",
            "train loss:  0.3133530020713806\n",
            "train loss:  0.14122796058654785\n",
            "train loss:  0.10769066959619522\n",
            "train loss:  0.28046736121177673\n",
            "train loss:  0.1615859866142273\n",
            "train loss:  0.23096585273742676\n",
            "train loss:  0.17539383471012115\n",
            "train loss:  0.2733722925186157\n",
            "train loss:  0.5319960117340088\n",
            "train loss:  0.19340656697750092\n",
            "train loss:  0.2411973923444748\n",
            "train loss:  0.19472888112068176\n",
            "train loss:  0.17721998691558838\n",
            "train loss:  0.41611993312835693\n",
            "train loss:  0.13151496648788452\n",
            "train loss:  0.3963499367237091\n",
            "train loss:  0.1719539314508438\n",
            "train loss:  0.23043011128902435\n",
            "train loss:  0.18310780823230743\n",
            "train loss:  0.3180984854698181\n",
            "train loss:  0.1764180213212967\n",
            "train loss:  0.2838939130306244\n",
            "train loss:  0.20875173807144165\n",
            "train loss:  0.4994966685771942\n",
            "train loss:  0.11039742082357407\n",
            "train loss:  0.3069301247596741\n",
            "train loss:  0.2874477207660675\n",
            "train loss:  0.2322172224521637\n",
            "train loss:  0.172178253531456\n",
            "train loss:  0.2565697133541107\n",
            "train loss:  0.4613465666770935\n",
            "train loss:  0.23671837151050568\n",
            "train loss:  0.2299622744321823\n",
            "train loss:  0.33378997445106506\n",
            "train loss:  0.1689976453781128\n",
            "train loss:  0.2199334353208542\n",
            "train loss:  0.23013927042484283\n",
            "train loss:  0.16502517461776733\n",
            "train loss:  0.2519070506095886\n",
            "train loss:  0.3359431326389313\n",
            "train loss:  0.11903546005487442\n",
            "train loss:  0.1347983479499817\n",
            "train loss:  0.22220583260059357\n",
            "train loss:  0.2453804612159729\n",
            "train loss:  0.3630507290363312\n",
            "train loss:  0.35988298058509827\n",
            "train loss:  0.327697217464447\n",
            "train loss:  0.27479010820388794\n",
            "train loss:  0.22235549986362457\n",
            "train loss:  0.30793237686157227\n",
            "train loss:  0.13451319932937622\n",
            "train loss:  0.18697209656238556\n",
            "train loss:  0.24925033748149872\n",
            "train loss:  0.3590396046638489\n",
            "train loss:  0.23941361904144287\n",
            "train loss:  0.2985236644744873\n",
            "train loss:  0.18643216788768768\n",
            "train loss:  0.22395481169223785\n",
            "train loss:  0.2788366675376892\n",
            "train loss:  0.20139268040657043\n",
            "train loss:  0.21962518990039825\n",
            "train loss:  0.1597646325826645\n",
            "train loss:  0.2885872423648834\n",
            "train loss:  0.24607884883880615\n",
            "train loss:  0.3489723205566406\n",
            "train loss:  0.12004449218511581\n",
            "train loss:  0.2235015332698822\n",
            "train loss:  0.1817760467529297\n",
            "train loss:  0.4591734707355499\n",
            "train loss:  0.1540212333202362\n",
            "train loss:  0.519000232219696\n",
            "train loss:  0.2081371694803238\n",
            "train loss:  0.2694278955459595\n",
            "train loss:  0.21990031003952026\n",
            "train loss:  0.2925255596637726\n",
            "train loss:  0.07899675518274307\n",
            "train loss:  0.25791776180267334\n",
            "train loss:  0.2776336073875427\n",
            "train loss:  0.13261349499225616\n",
            "train loss:  0.29694053530693054\n",
            "train loss:  0.16589653491973877\n",
            "train loss:  0.16301853954792023\n",
            "train loss:  0.2385808229446411\n",
            "train loss:  0.2136087566614151\n",
            "train loss:  0.23497357964515686\n",
            "train loss:  0.3338541090488434\n",
            "train loss:  0.29727497696876526\n",
            "train loss:  0.16826917231082916\n",
            "train loss:  0.14418286085128784\n",
            "train loss:  0.14729903638362885\n",
            "train loss:  0.19260694086551666\n",
            "train loss:  0.2456766963005066\n",
            "train loss:  0.12437832355499268\n",
            "train loss:  0.35474416613578796\n",
            "train loss:  0.3901921212673187\n",
            "train loss:  0.0723324716091156\n",
            "train loss:  0.2975430488586426\n",
            "train loss:  0.10437159240245819\n",
            "train loss:  0.2519376277923584\n",
            "train loss:  0.12814638018608093\n",
            "train loss:  0.06662017107009888\n",
            "train loss:  0.16424094140529633\n",
            "train loss:  0.13338427245616913\n",
            "train loss:  0.27647635340690613\n",
            "train loss:  0.20606620609760284\n",
            "train loss:  0.22281120717525482\n",
            "train loss:  0.19851979613304138\n",
            "train loss:  0.1354639083147049\n",
            "train loss:  0.27442851662635803\n",
            "train loss:  0.22546498477458954\n",
            "train loss:  0.34001457691192627\n",
            "train loss:  0.0826936885714531\n",
            "train loss:  0.1417865753173828\n",
            "train loss:  0.2621988356113434\n",
            "train loss:  0.302862286567688\n",
            "train loss:  0.18410946428775787\n",
            "train loss:  0.04573335126042366\n",
            "train loss:  0.13512350618839264\n",
            "train loss:  0.4656144380569458\n",
            "train loss:  0.2308657169342041\n",
            "train loss:  0.16962496936321259\n",
            "train loss:  0.10063331574201584\n",
            "train loss:  0.10097400099039078\n",
            "train loss:  0.07021058350801468\n",
            "train loss:  0.1595335751771927\n",
            "train loss:  0.19153611361980438\n",
            "train loss:  0.2700742185115814\n",
            "train loss:  0.22082901000976562\n",
            "train loss:  0.26129117608070374\n",
            "train loss:  0.11205901950597763\n",
            "train loss:  0.19292816519737244\n",
            "train loss:  0.28135937452316284\n",
            "train loss:  0.27145838737487793\n",
            "train loss:  0.2358066588640213\n",
            "train loss:  0.22379252314567566\n",
            "train loss:  0.12208923697471619\n",
            "train loss:  0.13579563796520233\n",
            "train loss:  0.23977141082286835\n",
            "train loss:  0.368612140417099\n",
            "train loss:  0.13885046541690826\n",
            "train loss:  0.30737733840942383\n",
            "train loss:  0.3537881672382355\n",
            "train loss:  0.15611279010772705\n",
            "train loss:  0.17707304656505585\n",
            "train loss:  0.38946297764778137\n",
            "train loss:  0.21956580877304077\n",
            "train loss:  0.332501620054245\n",
            "train loss:  0.2555544078350067\n",
            "train loss:  0.33248820900917053\n",
            "train loss:  0.18071754276752472\n",
            "train loss:  0.14087902009487152\n",
            "train loss:  0.26510003209114075\n",
            "train loss:  0.06734909862279892\n",
            "train loss:  0.16619527339935303\n",
            "train loss:  0.09247679263353348\n",
            "train loss:  0.11274424195289612\n",
            "train loss:  0.1442275196313858\n",
            "train loss:  0.20925965905189514\n",
            "train loss:  0.15643970668315887\n",
            "train loss:  0.2252025306224823\n",
            "train loss:  0.1541311889886856\n",
            "train loss:  0.05007806047797203\n",
            "train loss:  0.08654255419969559\n",
            "train loss:  0.3496347963809967\n",
            "train loss:  0.03527892753481865\n",
            "train loss:  0.07687975466251373\n",
            "train loss:  0.16564373672008514\n",
            "train loss:  0.06875581294298172\n",
            "train loss:  0.09266126900911331\n",
            "train loss:  0.08070117980241776\n",
            "train loss:  0.22184891998767853\n",
            "train loss:  0.16661690175533295\n",
            "train loss:  0.059377271682024\n",
            "train loss:  0.10315374284982681\n",
            "train loss:  0.13554571568965912\n",
            "train loss:  0.08278555423021317\n",
            "train loss:  0.14766694605350494\n",
            "train loss:  0.050683923065662384\n",
            "train loss:  0.21591778099536896\n",
            "train loss:  0.23194745182991028\n",
            "train loss:  0.1801605373620987\n",
            "train loss:  0.31992074847221375\n",
            "train loss:  0.15073233842849731\n",
            "train loss:  0.19630110263824463\n",
            "train loss:  0.06907500326633453\n",
            "train loss:  0.3656105399131775\n",
            "train loss:  0.09846783429384232\n",
            "train loss:  0.2492711842060089\n",
            "train loss:  0.12662966549396515\n",
            "train loss:  0.16225294768810272\n",
            "train loss:  0.2907969355583191\n",
            "train loss:  0.20776405930519104\n",
            "train loss:  0.46306291222572327\n",
            "train loss:  0.2614521086215973\n",
            "train loss:  0.271793395280838\n",
            "train loss:  0.3025507628917694\n",
            "train loss:  0.3649464249610901\n",
            "train loss:  0.20643071830272675\n",
            "train loss:  0.1529044657945633\n",
            "train loss:  0.3268905580043793\n",
            "train loss:  0.04481736570596695\n",
            "train loss:  0.2091084122657776\n",
            "train loss:  0.28237515687942505\n",
            "train loss:  0.11168204247951508\n",
            "train loss:  0.29865121841430664\n",
            "train loss:  0.33962368965148926\n",
            "train loss:  0.3212016820907593\n",
            "train loss:  0.07943672686815262\n",
            "train loss:  0.11936753243207932\n",
            "train loss:  0.14525145292282104\n",
            "train loss:  0.21340729296207428\n",
            "train loss:  0.22223778069019318\n",
            "train loss:  0.1879800707101822\n",
            "train loss:  0.1941213309764862\n",
            "train loss:  0.22620001435279846\n",
            "train loss:  0.22419844567775726\n",
            "train loss:  0.19159875810146332\n",
            "train loss:  0.11010938137769699\n",
            "train loss:  0.1987096071243286\n",
            "train loss:  0.16462744772434235\n",
            "train loss:  0.18531204760074615\n",
            "train loss:  0.15818358957767487\n",
            "train loss:  0.3001602292060852\n",
            "train loss:  0.13863706588745117\n",
            "train loss:  0.10608422756195068\n",
            "train loss:  0.2719338536262512\n",
            "train loss:  0.30659425258636475\n",
            "train loss:  0.15784721076488495\n",
            "train loss:  0.18474425375461578\n",
            "train loss:  0.29903459548950195\n",
            "train loss:  0.12878729403018951\n",
            "train loss:  0.12085530906915665\n",
            "train loss:  0.4568595588207245\n",
            "train loss:  0.1032184362411499\n",
            "train loss:  0.10605010390281677\n",
            "train loss:  0.18245306611061096\n",
            "train loss:  0.24652664363384247\n",
            "train loss:  0.29511451721191406\n",
            "train loss:  0.2905200123786926\n",
            "train loss:  0.193540558218956\n",
            "train loss:  0.22937195003032684\n",
            "train loss:  0.16497847437858582\n",
            "train loss:  0.351059228181839\n",
            "train loss:  0.32942238450050354\n",
            "train loss:  0.1965971291065216\n",
            "train loss:  0.1600746214389801\n",
            "train loss:  0.19225847721099854\n",
            "train loss:  0.20658661425113678\n",
            "train loss:  0.06635602563619614\n",
            "train loss:  0.21643856167793274\n",
            "train loss:  0.26081177592277527\n",
            "train loss:  0.3210250437259674\n",
            "train loss:  0.13416078686714172\n",
            "train loss:  0.2441883534193039\n",
            "train loss:  0.14400982856750488\n",
            "train loss:  0.19376593828201294\n",
            "train loss:  0.09197084605693817\n",
            "train loss:  0.09379144757986069\n",
            "train loss:  0.18576185405254364\n",
            "train loss:  0.26387691497802734\n",
            "train loss:  0.1345311999320984\n",
            "train loss:  0.20399504899978638\n",
            "train loss:  0.04921161010861397\n",
            "train loss:  0.12486033886671066\n",
            "train loss:  0.22613167762756348\n",
            "train loss:  0.24631837010383606\n",
            "train loss:  0.14754703640937805\n",
            "train loss:  0.3114601969718933\n",
            "train loss:  0.23841650784015656\n",
            "train loss:  0.22516435384750366\n",
            "train loss:  0.18385246396064758\n",
            "train loss:  0.07185917347669601\n",
            "train loss:  0.27506116032600403\n",
            "train loss:  0.2504682242870331\n",
            "train loss:  0.2537110149860382\n",
            "train loss:  0.3255182206630707\n",
            "train loss:  0.2135200798511505\n",
            "train loss:  0.16177061200141907\n",
            "train loss:  0.16238544881343842\n",
            "train loss:  0.11415271461009979\n",
            "train loss:  0.3605085015296936\n",
            "train loss:  0.21087564527988434\n",
            "train loss:  0.09600137174129486\n",
            "train loss:  0.27428632974624634\n",
            "train loss:  0.3436686396598816\n",
            "train loss:  0.1477867066860199\n",
            "train loss:  0.07068350911140442\n",
            "train loss:  0.11288617551326752\n",
            "train loss:  0.14325681328773499\n",
            "train loss:  0.24103952944278717\n",
            "train loss:  0.2741149067878723\n",
            "train loss:  0.2707546651363373\n",
            "train loss:  0.10654766857624054\n",
            "train loss:  0.0817994698882103\n",
            "train loss:  0.1228509172797203\n",
            "train loss:  0.1157837063074112\n",
            "train loss:  0.20613043010234833\n",
            "train loss:  0.31278759241104126\n",
            "train loss:  0.28835973143577576\n",
            "train loss:  0.3071345388889313\n",
            "train loss:  0.19553077220916748\n",
            "train loss:  0.3724067807197571\n",
            "train loss:  0.16817167401313782\n",
            "train loss:  0.24859294295310974\n",
            "train loss:  0.11398263275623322\n",
            "train loss:  0.23450137674808502\n",
            "train loss:  0.28553321957588196\n",
            "train loss:  0.19064593315124512\n",
            "train loss:  0.1380077749490738\n",
            "train loss:  0.16079676151275635\n",
            "train loss:  0.18306171894073486\n",
            "train loss:  0.20955000817775726\n",
            "train loss:  0.07697118818759918\n",
            "train loss:  0.12952320277690887\n",
            "train loss:  0.2138102799654007\n",
            "train loss:  0.1815967857837677\n",
            "train loss:  0.12980039417743683\n",
            "train loss:  0.2973288595676422\n",
            "train loss:  0.07347562164068222\n",
            "train loss:  0.04694897681474686\n",
            "train loss:  0.2341587096452713\n",
            "train loss:  0.1278805285692215\n",
            "train loss:  0.29748573899269104\n",
            "train loss:  0.141791433095932\n",
            "train loss:  0.09077438712120056\n",
            "train loss:  0.09769701957702637\n",
            "train loss:  0.08020827174186707\n",
            "train loss:  0.3115021288394928\n",
            "train loss:  0.23703689873218536\n",
            "train loss:  0.20897401869297028\n",
            "train loss:  0.22791574895381927\n",
            "train loss:  0.13531403243541718\n",
            "train loss:  0.16700679063796997\n",
            "train loss:  0.12966154515743256\n",
            "train loss:  0.11721768230199814\n",
            "train loss:  0.09447877109050751\n",
            "train loss:  0.255825936794281\n",
            "train loss:  0.07182522118091583\n",
            "train loss:  0.1170460432767868\n",
            "train loss:  0.18408749997615814\n",
            "train loss:  0.05506156012415886\n",
            "train loss:  0.05318437144160271\n",
            "train loss:  0.06216628849506378\n",
            "train loss:  0.16773547232151031\n",
            "train loss:  0.09833571314811707\n",
            "train loss:  0.14027740061283112\n",
            "train loss:  0.07438240200281143\n",
            "train loss:  0.040760159492492676\n",
            "train loss:  0.20494134724140167\n",
            "train loss:  0.1012323647737503\n",
            "train loss:  0.2431175857782364\n",
            "train loss:  0.08655820041894913\n",
            "train loss:  0.08111259341239929\n",
            "train loss:  0.2575375735759735\n",
            "train loss:  0.14621566236019135\n",
            "train loss:  0.4259626567363739\n",
            "train loss:  0.36838507652282715\n",
            "train loss:  0.17117498815059662\n",
            "train loss:  0.2646026313304901\n",
            "train loss:  0.06312264502048492\n",
            "train loss:  0.3445739448070526\n",
            "train loss:  0.43777915835380554\n",
            "train loss:  0.33869674801826477\n",
            "train loss:  0.10421517491340637\n",
            "train loss:  0.08576574921607971\n",
            "train loss:  0.10758325457572937\n",
            "train loss:  0.06681808084249496\n",
            "train loss:  0.25403568148612976\n",
            "train loss:  0.32498592138290405\n",
            "train loss:  0.0660889744758606\n",
            "train loss:  0.08386480063199997\n",
            "train loss:  0.0593407042324543\n",
            "train loss:  0.1607411652803421\n",
            "train loss:  0.1372637003660202\n",
            "train loss:  0.24035519361495972\n",
            "train loss:  0.13536900281906128\n",
            "train loss:  0.16612322628498077\n",
            "train loss:  0.17981326580047607\n",
            "train loss:  0.10705571621656418\n",
            "train loss:  0.060091640800237656\n",
            "train loss:  0.07234875112771988\n",
            "train loss:  0.14745086431503296\n",
            "train loss:  0.1388413906097412\n",
            "train loss:  0.19325107336044312\n",
            "train loss:  0.0855889618396759\n",
            "train loss:  0.0933331549167633\n",
            "train loss:  0.15673956274986267\n",
            "train loss:  0.3086497187614441\n",
            "train loss:  0.10439186543226242\n",
            "train loss:  0.12498263269662857\n",
            "train loss:  0.13549718260765076\n",
            "train loss:  0.10998725891113281\n",
            "train loss:  0.19773216545581818\n",
            "train loss:  0.07083098590373993\n",
            "train loss:  0.13978014886379242\n",
            "train loss:  0.06551845371723175\n",
            "train loss:  0.14123673737049103\n",
            "train loss:  0.17416030168533325\n",
            "train loss:  0.26678359508514404\n",
            "train loss:  0.2545219361782074\n",
            "train loss:  0.12973059713840485\n",
            "train loss:  0.24429865181446075\n",
            "train loss:  0.08631706237792969\n",
            "train loss:  0.14467734098434448\n",
            "train loss:  0.21032916009426117\n",
            "train loss:  0.16207489371299744\n",
            "train loss:  0.2936769127845764\n",
            "train loss:  0.05638818070292473\n",
            "train loss:  0.22829338908195496\n",
            "train loss:  0.1501891016960144\n",
            "train loss:  0.07898452132940292\n",
            "train loss:  0.07098855078220367\n",
            "train loss:  0.13650581240653992\n",
            "train loss:  0.22949017584323883\n",
            "train loss:  0.18003694713115692\n",
            "train loss:  0.13977612555027008\n",
            "train loss:  0.271210640668869\n",
            "train loss:  0.22681209444999695\n",
            "train loss:  0.09154035896062851\n",
            "train loss:  0.1622392237186432\n",
            "train loss:  0.041346121579408646\n",
            "train loss:  0.28956475853919983\n",
            "train loss:  0.1426587700843811\n",
            "train loss:  0.1436070054769516\n",
            "train loss:  0.2102450132369995\n",
            "train loss:  0.1364642083644867\n",
            "train loss:  0.14874088764190674\n",
            "train loss:  0.26543381810188293\n",
            "train loss:  0.09323154389858246\n",
            "train loss:  0.15377697348594666\n",
            "train loss:  0.1239723414182663\n",
            "train loss:  0.20108507573604584\n",
            "train loss:  0.2213267683982849\n",
            "train loss:  0.2926517724990845\n",
            "train loss:  0.29687872529029846\n",
            "train loss:  0.10505438596010208\n",
            "train loss:  0.08523775637149811\n",
            "train loss:  0.12852436304092407\n",
            "train loss:  0.15583649277687073\n",
            "train loss:  0.08972830325365067\n",
            "train loss:  0.03353266417980194\n",
            "train loss:  0.19461795687675476\n",
            "train loss:  0.25247544050216675\n",
            "train loss:  0.17090260982513428\n",
            "train loss:  0.10389678925275803\n",
            "train loss:  0.25495588779449463\n",
            "train loss:  0.22262795269489288\n",
            "train loss:  0.20677068829536438\n",
            "train loss:  0.26736193895339966\n",
            "train loss:  0.2706514894962311\n",
            "train loss:  0.21740642189979553\n",
            "train loss:  0.19691875576972961\n",
            "train loss:  0.15897175669670105\n",
            "train loss:  0.2806776762008667\n",
            "train loss:  0.2477167546749115\n",
            "train loss:  0.21449118852615356\n",
            "train loss:  0.17514757812023163\n",
            "train loss:  0.2267763614654541\n",
            "train loss:  0.27851876616477966\n",
            "train loss:  0.056614864617586136\n",
            "train loss:  0.1610170602798462\n",
            "train loss:  0.10353249311447144\n",
            "train loss:  0.17239832878112793\n",
            "train loss:  0.08780797570943832\n",
            "train loss:  0.08597654849290848\n",
            "train loss:  0.1478084772825241\n",
            "train loss:  0.06759262084960938\n",
            "train loss:  0.08172892034053802\n",
            "train loss:  0.06952580064535141\n",
            "train loss:  0.30138564109802246\n",
            "train loss:  0.11698680371046066\n",
            "train loss:  0.1618596315383911\n",
            "train loss:  0.12326734513044357\n",
            "train loss:  0.30858948826789856\n",
            "train loss:  0.14816530048847198\n",
            "train loss:  0.10780640691518784\n",
            "train loss:  0.1869467943906784\n",
            "train loss:  0.13300400972366333\n",
            "train loss:  0.200777068734169\n",
            "train loss:  0.1217506006360054\n",
            "train loss:  0.12321586161851883\n",
            "train loss:  0.15236642956733704\n",
            "train loss:  0.18518559634685516\n",
            "train loss:  0.21373841166496277\n",
            "train loss:  0.05857579782605171\n",
            "train loss:  0.2906738519668579\n",
            "train loss:  0.29771193861961365\n",
            "train loss:  0.16369903087615967\n",
            "train loss:  0.07567489892244339\n",
            "train loss:  0.030922552570700645\n",
            "train loss:  0.2131659984588623\n",
            "train loss:  0.15705758333206177\n",
            "train loss:  0.16505087912082672\n",
            "train loss:  0.1933116763830185\n",
            "train loss:  0.06680135428905487\n",
            "train loss:  0.07235302031040192\n",
            "train loss:  0.10111581534147263\n",
            "train loss:  0.11522091925144196\n",
            "train loss:  0.14858339726924896\n",
            "train loss:  0.07447362691164017\n",
            "train loss:  0.1483284831047058\n",
            "train loss:  0.15624789893627167\n",
            "train loss:  0.10438288748264313\n",
            "train loss:  0.3855789303779602\n",
            "train loss:  0.0915776938199997\n",
            "train loss:  0.06915652751922607\n",
            "train loss:  0.23834939301013947\n",
            "train loss:  0.04903737083077431\n",
            "train loss:  0.1927213817834854\n",
            "train loss:  0.2447046935558319\n",
            "train loss:  0.09127220511436462\n",
            "train loss:  0.3514145016670227\n",
            "train loss:  0.1497926563024521\n",
            "train loss:  0.08707103133201599\n",
            "train loss:  0.14107027649879456\n",
            "train loss:  0.12531553208827972\n",
            "train loss:  0.2180231213569641\n",
            "train loss:  0.20045334100723267\n",
            "train loss:  0.12800191342830658\n",
            "train loss:  0.13981766998767853\n",
            "train loss:  0.207551971077919\n",
            "train loss:  0.09786485135555267\n",
            "train loss:  0.1702578216791153\n",
            "train loss:  0.07117924839258194\n",
            "train loss:  0.1711806207895279\n",
            "train loss:  0.21396282315254211\n",
            "train loss:  0.08580932021141052\n",
            "train loss:  0.06505653262138367\n",
            "train loss:  0.3666830062866211\n",
            "train loss:  0.11866907775402069\n",
            "train loss:  0.23284687101840973\n",
            "train loss:  0.26349329948425293\n",
            "train loss:  0.11935348808765411\n",
            "train loss:  0.06226741895079613\n",
            "train loss:  0.1282792091369629\n",
            "train loss:  0.24744319915771484\n",
            "train loss:  0.20423506200313568\n",
            "train loss:  0.2077072411775589\n",
            "train loss:  0.052687130868434906\n",
            "train loss:  0.18202073872089386\n",
            "train loss:  0.0649660974740982\n",
            "train loss:  0.1592230498790741\n",
            "train loss:  0.23630689084529877\n",
            "train loss:  0.09602722525596619\n",
            "train loss:  0.33860453963279724\n",
            "train loss:  0.12903209030628204\n",
            "train loss:  0.0912964791059494\n",
            "train loss:  0.09238285571336746\n",
            "train loss:  0.05770391225814819\n",
            "train loss:  0.1593339890241623\n",
            "train loss:  0.0859941616654396\n",
            "train loss:  0.07466359436511993\n",
            "train loss:  0.21985381841659546\n",
            "train loss:  0.08066832274198532\n",
            "train loss:  0.07107802480459213\n",
            "train loss:  0.11039654165506363\n",
            "train loss:  0.025970153510570526\n",
            "train loss:  0.1582963615655899\n",
            "train loss:  0.035059425979852676\n",
            "train loss:  0.09403292089700699\n",
            "train loss:  0.35810422897338867\n",
            "train loss:  0.24499940872192383\n",
            "train loss:  0.10797584056854248\n",
            "train loss:  0.16734299063682556\n",
            "train loss:  0.1317860186100006\n",
            "train loss:  0.0938202440738678\n",
            "train loss:  0.20701725780963898\n",
            "train loss:  0.2105238288640976\n",
            "train loss:  0.1234188973903656\n",
            "train loss:  0.16871517896652222\n",
            "train loss:  0.1062404215335846\n",
            "train loss:  0.16415365040302277\n",
            "train loss:  0.18101894855499268\n",
            "train loss:  0.1119978204369545\n",
            "train loss:  0.21751734614372253\n",
            "train loss:  0.07855626195669174\n",
            "train loss:  0.17321236431598663\n",
            "train loss:  0.24666361510753632\n",
            "train loss:  0.08944162726402283\n",
            "train loss:  0.12361188232898712\n",
            "train loss:  0.10955675691366196\n",
            "train loss:  0.17005349695682526\n",
            "train loss:  0.20510776340961456\n",
            "train loss:  0.1430516391992569\n",
            "train loss:  0.12016542255878448\n",
            "train loss:  0.1144341304898262\n",
            "train loss:  0.04105232283473015\n",
            "train loss:  0.14061743021011353\n",
            "train loss:  0.2175254374742508\n",
            "train loss:  0.11928489804267883\n",
            "train loss:  0.15574946999549866\n",
            "train loss:  0.16713979840278625\n",
            "train loss:  0.2526858448982239\n",
            "train loss:  0.18949849903583527\n",
            "train loss:  0.2119928002357483\n",
            "train loss:  0.19280822575092316\n",
            "train loss:  0.11278050392866135\n",
            "train loss:  0.21965229511260986\n",
            "train loss:  0.32411056756973267\n",
            "train loss:  0.06381457298994064\n",
            "train loss:  0.1656561940908432\n",
            "train loss:  0.1525615006685257\n",
            "train loss:  0.14602822065353394\n",
            "train loss:  0.16918480396270752\n",
            "train loss:  0.04371929541230202\n",
            "train loss:  0.1678897887468338\n",
            "train loss:  0.28969985246658325\n",
            "train loss:  0.20086801052093506\n",
            "train loss:  0.15014609694480896\n",
            "train loss:  0.06671002507209778\n",
            "train loss:  0.14078910648822784\n",
            "train loss:  0.3535890579223633\n",
            "train loss:  0.07104839384555817\n",
            "train loss:  0.06301391124725342\n",
            "train loss:  0.05042695626616478\n",
            "train loss:  0.11304189264774323\n",
            "train loss:  0.11956778168678284\n",
            "train loss:  0.22913014888763428\n",
            "train loss:  0.14087752997875214\n",
            "train loss:  0.19286863505840302\n",
            "train loss:  0.12361052632331848\n",
            "train loss:  0.14462974667549133\n",
            "train loss:  0.20678038895130157\n",
            "train loss:  0.07749726623296738\n",
            "train loss:  0.34896042943000793\n",
            "train loss:  0.08431417495012283\n",
            "train loss:  0.1627068966627121\n",
            "train loss:  0.17457683384418488\n",
            "train loss:  0.159147709608078\n",
            "train loss:  0.3427281379699707\n",
            "train loss:  0.02146151289343834\n",
            "train loss:  0.11815228313207626\n",
            "train loss:  0.0641341283917427\n",
            "train loss:  0.06467164307832718\n",
            "train loss:  0.1074608713388443\n",
            "train loss:  0.15476274490356445\n",
            "train loss:  0.2036362886428833\n",
            "train loss:  0.11297454684972763\n",
            "train loss:  0.05050094053149223\n",
            "train loss:  0.17459827661514282\n",
            "train loss:  0.1486659198999405\n",
            "train loss:  0.13536977767944336\n",
            "train loss:  0.14722272753715515\n",
            "train loss:  0.11828634142875671\n",
            "val_loss:  0.1502876579761505\n"
          ]
        }
      ]
    }
  ]
}