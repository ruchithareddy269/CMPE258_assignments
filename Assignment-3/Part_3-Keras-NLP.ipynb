{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCHHL5HZP5Hs"
      },
      "source": [
        "# Use Keras-NLP for various nlp tasks\n",
        "\n",
        "\n",
        "Inference with a pretrained classifier\n",
        "\n",
        "Fine tuning a pretrained backbone\n",
        "\n",
        "Fine tuning with user-controlled preprocessing\n",
        "\n",
        "Fine tuning a custom modes  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6-Olz27P5Hw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "A natural language processing library called KerasNLP helps users at every stage of the development process. Our workflows are constructed from modular parts that, when utilized, have cutting-edge predefined weights and architectures.\n",
        "\n",
        "unconventional and are readily adaptable to require additional control.\n",
        "\n",
        "This library is an extension of the core Keras API; all high-level modules are\n",
        "[`Layers`](/api/layers/) or [`Models`](/api/models/). If you are familiar with Keras,\n",
        "congratulations! You already understand most of KerasNLP.\n",
        "\n",
        "KerasNLP uses Keras 3 to work with any of TensorFlow, Pytorch and Jax. In the\n",
        "guide below, we will use the `jax` backend for training our models, and\n",
        "[tf.data](https://www.tensorflow.org/guide/data) for efficiently running our\n",
        "input preprocessing. But feel free to mix things up! This guide runs in\n",
        "TensorFlow or PyTorch backends with zero changes, simply update the\n",
        "`KERAS_BACKEND` below.\n",
        "\n",
        "This demonstrates modular approach using a sentiment analysis example at 4\n",
        "levels of complexity:\n",
        "\n",
        "* Inference with a pretrained classifier\n",
        "* Fine tuning a pretrained backbone\n",
        "* Fine tuning with user-controlled preprocessing\n",
        "* Fine tuning a custom model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tf-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ5hH5uGUQhN",
        "outputId": "0052b11d-4b06-4a27-c21b-9d24b1fcef86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
            "Requirement already satisfied: tensorflow<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade tensorflow==2.16.1"
      ],
      "metadata": {
        "id": "GaqH3OmpUi0o"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xwNns72zP5Hx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1230cb-a2c5-478b-ae24-3c6dceaacd67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w33Mgl2YP5Hy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "import keras_nlp\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.set_image_data_format('channels_last')\n",
        "\n",
        "# Import necessary modules from Keras and TensorFlow\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.mixed_precision as mixed_precision\n",
        "\n",
        "# Set the global policy to use mixed precision for faster training\n",
        "# This is specifically tailored to TensorFlow's handling of mixed precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n"
      ],
      "metadata": {
        "id": "sEeZmjBohJMX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l26cAeUP5Hz"
      },
      "source": [
        "The highest level API in keras_nlp.models provides a comprehensive suite of modules for handling various natural language processing (NLP) tasks. Each module within this API focuses on a specific aspect of the NLP pipeline, from converting strings to tokens to generating task-specific outputs. Here's a summary of the key modules:\n",
        "\n",
        "Tokenizer: keras_nlp.models.XXTokenizer\n",
        "\n",
        "Converts strings to sequences of token IDs.\n",
        "Essential for mapping raw strings to a manageable number of tokens.\n",
        "Inherits from keras.layers.Layer.\n",
        "\n",
        "\n",
        "Preprocessor: keras_nlp.models.XXPreprocessor\n",
        "\n",
        "Converts strings to preprocessed tensors used by the backbone.\n",
        "Incorporates special tokens and tensors for understanding input sequences.\n",
        "Utilizes a tokenizer.\n",
        "Inherits from keras.layers.Layer.\n",
        "\n",
        "\n",
        "Backbone: keras_nlp.models.XXBackbone\n",
        "\n",
        "Converts preprocessed tensors to dense features.\n",
        "Distills input tokens into dense features for downstream tasks.\n",
        "Inherits from keras.Model.\n",
        "\n",
        "\n",
        "Task Model: e.g., keras_nlp.models.XXClassifier\n",
        "\n",
        "Converts strings to task-specific output, such as classification probabilities.\n",
        "Combines preprocessing, backbone, and task-specific layers.\n",
        "Requires fine-tuning on labeled data.\n",
        "Inherits from keras.Model.\n",
        "The modular hierarchy, exemplified by BertClassifier, emphasizes compositional relationships between the modules, allowing for flexible and customizable NLP workflows. All modules offer a from_preset() method for instantiating the class with preset architecture and weights, simplifying usage.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUVxcZ17P5Hz"
      },
      "source": [
        "## Data\n",
        "The AG News dataset is a collection of news articles categorized into four classes: World, Sports, Business, and Science/Technology. Here's a breakdown of the code and what it does:\n",
        "\n",
        "**Loading the Dataset:**\n",
        "\n",
        "The dataset is loaded using tfds.load() with the name 'ag_news_subset'. This dataset contains a subset of the AG News dataset.\n",
        "The dataset is split into training and testing sets using the split parameter.\n",
        "\n",
        "**Batching the Dataset:**\n",
        "\n",
        "The training and testing datasets are batched using the batch() method. The BATCH_SIZE is set to 16, meaning each batch will contain 16 examples.\n",
        "Decoding Text:\n",
        "\n",
        "The decode_text() function is defined to decode text from a tensor. It takes a text tensor and a label tensor as input and returns the decoded text and label.\n",
        "\n",
        "**Inspecting the Dataset:**\n",
        "\n",
        "The code then inspects the first review in the training set. It iterates over the training set, decodes the text and label tensors using the decode_text() function, and prints them out.\n",
        "Additionally, it inspects the first review by using the unbatch().take(1).get_single_element() method. This returns the first example in the dataset without decoding.\n",
        "Overall, the code loads the AG News dataset, batches it, and provides a glimpse of the data by inspecting the first review. The dataset is commonly used for text classification tasks such as sentiment analysis or topic classification.\n",
        "\n",
        "We load the data using `keras.utils.text_dataset_from_directory`, which utilizes the\n",
        "powerful `tf.data.Dataset` format for examples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll4lZm9jjYZn",
        "outputId": "56c2eea0-b2b1-4448-c50d-67dec296de8b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "BaViget7nlUU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "ag_news, info = tfds.load('ag_news_subset', with_info=True)"
      ],
      "metadata": {
        "id": "_8oSKJ1BPlNU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ5weisDQeHc",
        "outputId": "f763040a-ed18-4551-8903-2966d76e3613"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.25.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.3.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.10.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2024.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.63.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the AG News dataset\n",
        "ag_news_train, ag_news_test = tfds.load('ag_news_subset', split=['train', 'test'], as_supervised=True)\n",
        "BATCH_SIZE = 16\n",
        "# Batch the datasets\n",
        "ag_news_train = ag_news_train.batch(BATCH_SIZE)\n",
        "ag_news_test = ag_news_test.batch(BATCH_SIZE)\n",
        "\n",
        "# Function to decode the text from a tensor\n",
        "def decode_text(text_tensor, label_tensor):\n",
        "    text = text_tensor.numpy().decode('utf-8')\n",
        "    label = label_tensor.numpy()\n",
        "    return text, label\n",
        "\n",
        "# Inspect the first review of the training set\n",
        "for text_tensor, label_tensor in ag_news_train.unbatch().take(1):\n",
        "    text, label = decode_text(text_tensor, label_tensor)\n",
        "    print(f\"Text: {text}\\nLabel: {label}\")\n",
        "\n",
        "# Inspect first review\n",
        "# Format is (review text tensor, label tensor)\n",
        "print(ag_news_train.unbatch().take(1).get_single_element())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIyo7olJQjcw",
        "outputId": "e257394a-ea10-4f87-dd8d-cd4d57f6d38a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.\n",
            "Label: 3\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.'>, <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the AG News dataset\n",
        "ag_news_train, ag_news_test = tfds.load('ag_news_subset', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Take a subset of the dataset for both training and testing\n",
        "# Here, `.take(n)` means we take only the first 'n' batches\n",
        "# For example, if BATCH_SIZE = 16 and n = 100, we take 1600 examples\n",
        "SUBSET_SIZE = 100  # Number of batches to take\n",
        "ag_news_train_subset = ag_news_train.batch(BATCH_SIZE).take(SUBSET_SIZE)\n",
        "ag_news_test_subset = ag_news_test.batch(BATCH_SIZE).take(SUBSET_SIZE)\n",
        "\n",
        "# Function to decode the text from a tensor\n",
        "def decode_text(text_tensor, label_tensor):\n",
        "    text = text_tensor.numpy().decode('utf-8')\n",
        "    label = label_tensor.numpy()\n",
        "    return text, label\n",
        "\n",
        "# Inspect the first review of the training set subset\n",
        "for text_tensor, label_tensor in ag_news_train_subset.unbatch().take(1):\n",
        "    text, label = decode_text(text_tensor, label_tensor)\n",
        "    print(f\"Text: {text}\\nLabel: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqObmnBmVLxp",
        "outputId": "0c9c6534-f124-406c-f954-deffedf87599"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.\n",
            "Label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCkJSNq-P5H0"
      },
      "source": [
        "## **1) Inference with a pretrained classifier**\n",
        "\n",
        "\n",
        "A task is the highest level module in KerasNLP. A task is a {keras.Model} made up of task-specific layers and a **backbone** model, which is typically pretrained.\n",
        "Using `keras_nlp.models.BertClassifier}, here's an example.\n",
        "\n",
        "**Note**: The logits for each class are the outputs (e.g., {[0, 0]} indicates a 50% chance of positive). For binary classification, the result is [positive, negative]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r0bIOEyjP5H1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddd0829-0289-4026-d714-6489b155989d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 2 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n",
            "  trackable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.539,  1.542]], dtype=float16)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "classifier.predict([\"I love modular workflows in keras-nlp!\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkqrGswWP5H1"
      },
      "source": [
        "All **tasks** have a `from_preset` method that constructs a `keras.Model` instance with\n",
        "preset preprocessing, architecture and weights. This means that we can pass raw strings\n",
        "in any format accepted by a `keras.Model` and get output specific to our task.\n",
        "\n",
        "This particular **preset** is a `\"bert_tiny_uncased_en\"` **backbone** fine-tuned on\n",
        "`sst2`, another movie review sentiment analysis (this time from Rotten Tomatoes). We use\n",
        "the `tiny` architecture for demo purposes, but larger models are recommended for SoTA\n",
        "performance. For all the task-specific presets available for `BertClassifier`, see\n",
        "our keras.io [models page](https://keras.io/api/keras_nlp/models/).\n",
        "\n",
        "Let's evaluate our classifier on the IMDB dataset. You will note we don't need to\n",
        "call `keras.Model.compile` here. All **task** models like `BertClassifier` ship with\n",
        "compilation defaults, meaning we can just call `keras.Model.evaluate` directly. You\n",
        "can always call compile as normal to override these defaults (e.g. to add new metrics).\n",
        "\n",
        "The output below is [loss, accuracy],"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `classifier` is your TensorFlow/Keras model\n",
        "eval_results = classifier.evaluate(ag_news_test_subset)\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hb0Gg_1qpux",
        "outputId": "8a69029c-20c8-4b99-808d-6b2b6b11e188"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - loss: 0.5685 - sparse_categorical_accuracy: 0.2733\n",
            "[0.548656702041626, 0.2862499952316284]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtgECHBpP5H1"
      },
      "source": [
        "Our result is around 29% accuracy without training anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opzCeJHYP5H1"
      },
      "source": [
        "## 2)Fine tuning a pretrained BERT backbone\n",
        "\n",
        "Performance can be enhanced by fine-tuning a custom classifier when labeled text relevant to our task is available. Rotten Tomatoes data should not outperform IMDB data in terms of predicting the sentiment of IMDB reviews! Furthermore, there won't be any relevant pretrained models available for many tasks (like classifying customer reviews).\n",
        "\n",
        "\n",
        "With the exception of requesting a **preset** for the **backbone**-only model rather than the complete classifier, the fine-tuning procedure is nearly the same as it was previously described. A **task** {Model} will randomly initialize all task-specific layers in order to get ready for training when given a **backbone** **preset**.\n",
        "\n",
        "Visit our keras.io [models page](https://keras.io/api/keras_nlp/models/) to view all of the **backbone** presets that are available for {BertClassifier}.\n",
        "\n",
        "\n",
        "To train your classifier, use `keras.Model.fit` as with any other\n",
        "`keras.Model`. As with our inference example, we can rely on the compilation\n",
        "defaults for the **task** and skip `keras.Model.compile`. As preprocessing is\n",
        "included, we again pass the raw data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=2,\n",
        ")\n",
        "classifier.fit(\n",
        "    ag_news_train_subset,\n",
        "    validation_data=ag_news_test_subset,\n",
        "    epochs=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fw1BSHpVS2X",
        "outputId": "58266e57-ec73-43e8-a5eb-c3470ed8644d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 5s/step - loss: 0.3134 - sparse_categorical_accuracy: 0.4166 - val_loss: 0.1645 - val_sparse_categorical_accuracy: 0.5081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79fb5e8ebd60>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJfQnT3BP5H2"
      },
      "source": [
        "Here we see a significant lift in validation accuracy (0.29 to 0.41) with a 1 epoch of\n",
        "training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW49qUKFP5H2"
      },
      "source": [
        "## 3)Fine tuning with user-controlled preprocessing\n",
        "\n",
        "\n",
        "In some advanced training scenarios, users might prefer to be directly in charge of preprocessing. For larger datasets, examples can be preprocessed beforehand and saved to disk or by a different worker pool using `tf.data.experimental.service}.\n",
        "\n",
        "In other cases, special preprocessing is needed to handle the inputs.\n",
        "\n",
        "Pass {preprocessor=None} or a custom `BertPreprocessor} to the constructor of a **task** {Model} to prevent automatic preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhvS7pTWP5H2"
      },
      "source": [
        "### Separate preprocessing from the same preset\n",
        "\n",
        "Each model architecture has a parallel **preprocessor** `Layer` with its own\n",
        "`from_preset` constructor. Using the same **preset** for this `Layer` will return the\n",
        "matching **preprocessor** as the **task**.\n",
        "\n",
        "In this workflow we train the model over three epochs using `tf.data.Dataset.cache()`,\n",
        "which computes the preprocessing once and caches the result before fitting begins.\n",
        "\n",
        "**Note:** we can use `tf.data` for preprocessing while running on the\n",
        "Jax or PyTorch backend. The input dataset will automatically be converted to\n",
        "backend native tensor types during fit. In fact, given the efficiency of `tf.data`\n",
        "for running preprocessing, this is good practice on all backends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8irW3hj5P5H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af3abbb-5d30-4ae6-88b1-83297fa6da77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 5s/step - loss: 0.3123 - sparse_categorical_accuracy: 0.4051 - val_loss: 0.1555 - val_sparse_categorical_accuracy: 0.5081\n",
            "Epoch 2/3\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 5s/step - loss: 0.1356 - sparse_categorical_accuracy: 0.4963 - val_loss: 0.1000 - val_sparse_categorical_accuracy: 0.4975\n",
            "Epoch 3/3\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 4s/step - loss: 0.0694 - sparse_categorical_accuracy: 0.5052 - val_loss: 0.0554 - val_sparse_categorical_accuracy: 0.5113\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79fb5f69b280>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "# Apply the preprocessor to every sample of train and test data using `map()`.\n",
        "# `tf.data.AUTOTUNE` and `prefetch()` are options to tune performance, see\n",
        "# https://www.tensorflow.org/guide/data_performance for details.\n",
        "\n",
        "# Note: only call `cache()` if you training data fits in CPU memory!\n",
        "ag_train_cached = (\n",
        "    ag_news_train_subset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "ag_test_cached = (\n",
        "    ag_news_test_subset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n",
        ")\n",
        "classifier.fit(\n",
        "    ag_train_cached,\n",
        "    validation_data=ag_test_cached,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlO1YICQP5H2"
      },
      "source": [
        "After three epochs, our validation accuracy has only increased to 0.51. This is both a\n",
        "function of the small size of our dataset and our model. To exceed 90% accuracy, try\n",
        "larger **presets** such as  `\"bert_base_en_uncased\"`. For all the **backbone** presets\n",
        "available for `BertClassifier`, see our keras.io [models page](https://keras.io/api/keras_nlp/models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu_VEDdvP5H3"
      },
      "source": [
        "## 4)Fine tuning with a custom model\n",
        "\n",
        "There might not be a suitable **task** {Model} for more complex applications. Here, we offer direct access to the **backbone** {Model}, which can be assembled using unique {Layer}s and has its own `from_preset} constructor. Visit our [transfer learning guide](https://keras.io/guides/transfer_learning/) for more information and detailed examples.\n",
        "\n",
        "While automatic preprocessing is not included in a **backbone** {Model}, it can be paired with a compatible **preprocessor** by using the same **preset** as demonstrated in the workflow before.\n",
        "\n",
        "In order to adjust to the new input, we test the idea of freezing our backbone model and adding two trainable transformer layers in this workflow.\n",
        "\n",
        "**Note**: Since we are using the sequence output from BERT, we can disregard the warning regarding gradients for the {pooled_dense} layer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
        "\n",
        "ag_train_preprocessed = (\n",
        "    ag_news_train_subset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "ag_test_preprocessed = (\n",
        "    ag_news_test_subset.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "    sequence = keras_nlp.layers.TransformerEncoder(\n",
        "        num_heads=2,\n",
        "        intermediate_dim=512,\n",
        "        dropout=0.1,\n",
        "    )(sequence)\n",
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(5e-5),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    ag_train_preprocessed,\n",
        "    validation_data=ag_test_preprocessed,\n",
        "    epochs=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "ZbIQD21Sj9c9",
        "outputId": "c72cb911-aa35-40c1-f70b-0fbbf933758e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m4,385,920\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mBertBackbone\u001b[0m)            │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]            │                │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m198,272\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m198,272\u001b[0m │ transformer_encoder_2… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_13 (\u001b[38;5;33mGetItem\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ transformer_encoder_3… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m258\u001b[0m │ get_item_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)            │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]            │                │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ transformer_encoder_2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_encoder_3… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ get_item_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,782,722\u001b[0m (18.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,782,722</span> (18.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m396,802\u001b[0m (1.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">396,802</span> (1.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,385,920\u001b[0m (16.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> (16.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 6s/step - loss: 0.3071 - sparse_categorical_accuracy: 0.4037 - val_loss: 0.0886 - val_sparse_categorical_accuracy: 0.4950\n",
            "Epoch 2/3\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 6s/step - loss: 0.1064 - sparse_categorical_accuracy: 0.4793 - val_loss: 0.0682 - val_sparse_categorical_accuracy: 0.5063\n",
            "Epoch 3/3\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 6s/step - loss: 0.0823 - sparse_categorical_accuracy: 0.4937 - val_loss: 0.0727 - val_sparse_categorical_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79fb09904460>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cREN9sP5H3"
      },
      "source": [
        "This model achieves reasonable accuracy despite having only 10% of the trainable parameters\n",
        "of our `BertClassifier` model. Each training step takes about 1/3 of the time---even\n",
        "accounting for cached preprocessing.\n",
        "\n",
        "and the loss also reduced after 3 epochs and accuracy increased to 0.5% from 0.3(initial accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}